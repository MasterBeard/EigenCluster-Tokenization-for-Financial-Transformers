{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKvu3NqkWeHCtJ18Wb6VLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasterBeard/EigenCluster-Tokenization-for-Financial-Transformers/blob/main/ICLR_review3_VQ_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT6YewYvJ8q9",
        "outputId": "0827dfd1-c4f1-4de3-a0cc-efd97496bf64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['^SETI']: YFPricesMissingError('possibly delisted; no price data found  (1d 2011-01-01 -> 2020-12-31)')\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (23092, 44)\n",
            "Train norm_features shape: (23092, 44)\n",
            "Train labels shape: (23092,)\n",
            "Validation features shape: (2410, 44)\n",
            "Validation norm_features shape: (2410, 44)\n",
            "Validation labels shape: (2410,)\n",
            "Test features shape: (69348, 44)\n",
            "Test norm_features shape: (69348, 44)\n",
            "Test labels shape: (69348,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# Define index codes - smaller set for train/val\n",
        "train_val_tickers = {\n",
        "    'SPX': '^GSPC',     # S&P 500\n",
        "    'IXIC': '^IXIC',    # NASDAQ Composite\n",
        "    'HSI': '^HSI',      # Hang Seng Index\n",
        "    'DJI': '^DJI',      # Dow Jones Industrial Average\n",
        "    'FCHI': '^FCHI',    # CAC 40\n",
        "    'DAXI': '^GDAXI',   # DAX\n",
        "    'N225': '^N225',    # Nikkei 225\n",
        "    'KS11': '^KS11',    # KOSPI\n",
        "    'SENSEX': '^BSESN', # BSE Sensex\n",
        "    'STOXX50': '^STOXX50E',  # EURO STOXX 50\n",
        "    #'SSEC': '000001.SS', # Shanghai Composite\n",
        "    #'SZSC': '399001.SZ', # Shenzhen Component\n",
        "}\n",
        "\n",
        "# Full set of indices for test\n",
        "test_tickers = {\n",
        "    # North America\n",
        "    'SPX': '^GSPC',     # S&P 500\n",
        "    'IXIC': '^IXIC',    # NASDAQ Composite\n",
        "    'DJI': '^DJI',      # Dow Jones Industrial Average\n",
        "    'RUT': '^RUT',      # Russell 2000\n",
        "    #'VIX': '^VIX',      # CBOE Volatility Index\n",
        "    'TSX': '^GSPTSE',   # S&P/TSX Composite (Canada)\n",
        "\n",
        "    # Europe\n",
        "    'FTSE': '^FTSE',    # FTSE 100 (UK)\n",
        "    'DAXI': '^GDAXI',   # DAX (Germany)\n",
        "    'CAC': '^FCHI',     # CAC 40 (France)\n",
        "    'STOXX50': '^STOXX50E', # EURO STOXX 50\n",
        "    'IBEX': '^IBEX',    # IBEX 35 (Spain)\n",
        "    'FTMIB': 'FTSEMIB.MI', # FTSE MIB (Italy)\n",
        "    'SMI': '^SSMI',     # Swiss Market Index\n",
        "\n",
        "    # Asia\n",
        "    'HSI': '^HSI',      # Hang Seng (Hong Kong)\n",
        "    'N225': '^N225',    # Nikkei 225 (Japan)\n",
        "    'KS11': '^KS11',    # KOSPI (South Korea)\n",
        "    'TWII': '^TWII',    # TSEC weighted index (Taiwan)\n",
        "    'STI': '^STI',      # Straits Times Index (Singapore)\n",
        "    'JKSE': '^JKSE',    # Jakarta Composite (Indonesia)\n",
        "    'SET': '^SETI',     # SET Index (Thailand)\n",
        "    'NIFTY50': '^NSEI', # NIFTY 50 (India)\n",
        "    'SENSEX': '^BSESN', # BSE Sensex (India)\n",
        "\n",
        "    # Oceania\n",
        "    'AXJO': '^AXJO',    # S&P/ASX 200 (Australia)\n",
        "    'NZ50': '^NZ50',    # S&P/NZX 50 (New Zealand)\n",
        "\n",
        "    # Emerging Markets\n",
        "    'MERV': '^MERV',    # MERVAL (Argentina)\n",
        "    'BOVESPA': '^BVSP', # Bovespa (Brazil)\n",
        "    'IPC': '^MXX',      # IPC (Mexico)\n",
        "\n",
        "    # China\n",
        "    'SSEC': '000001.SS', # Shanghai Composite\n",
        "    'SZSC': '399001.SZ', # Shenzhen Component\n",
        "\n",
        "    # Global/Regional\n",
        "    'EEM': 'EEM',       # MSCI Emerging Markets ETF\n",
        "}\n",
        "\n",
        "idm = -2\n",
        "\n",
        "date_ranges = {\n",
        "    'train': (\"2000-01-01\", \"2009-12-31\"),\n",
        "    'val': (\"2010-01-02\", \"2010-12-31\"),\n",
        "    'test': (\"2011-01-01\", \"2020-12-31\")\n",
        "}\n",
        "#date_ranges = {\n",
        "    #'train': (\"2004-01-01\", \"2013-12-31\"),\n",
        "    #'val': (\"2014-01-02\", \"2014-12-31\"),\n",
        "    #'test': (\"2015-01-01\", \"2024-12-31\")\n",
        "#}\n",
        "\n",
        "# Initialize data storage\n",
        "data_splits = {split: {'features': [], 'norm_features': [], 'labels': []} for split in date_ranges}\n",
        "\n",
        "# Window length\n",
        "window_size = 11\n",
        "\n",
        "# Process train and val data (using smaller set of indices)\n",
        "for split in ['train', 'val']:\n",
        "    start_date, end_date = date_ranges[split]\n",
        "    index_data = {}\n",
        "    for name, ticker in train_val_tickers.items():\n",
        "        if ticker.endswith('.SZ') or ticker.endswith('.SS'):\n",
        "            index_data[name] = yf.Ticker(ticker).history(start=start_date, end=end_date, auto_adjust=True)\n",
        "        else:\n",
        "            index_data[name] = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True)\n",
        "\n",
        "    # Create feature vectors and labels\n",
        "    for index_name, data in index_data.items():\n",
        "        if data.empty:\n",
        "            continue\n",
        "\n",
        "        # Fix multi-level column names if needed\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "        open_values = data['Open'].dropna().values\n",
        "        close_values = data['Close'].dropna().values\n",
        "        low_values = data['Low'].dropna().values\n",
        "        high_values = data['High'].dropna().values\n",
        "\n",
        "        for start in range(len(data) - window_size + 1):\n",
        "            open_row = open_values[start:start + window_size]\n",
        "            low_row = low_values[start:start + window_size]\n",
        "            high_row = high_values[start:start + window_size]\n",
        "            close_row = close_values[start:start + window_size]\n",
        "\n",
        "            # Build feature vector (unnormalized)\n",
        "            combined = np.array([\n",
        "                val for i in range(window_size)\n",
        "                for val in (open_row[i], low_row[i], high_row[i], close_row[i])\n",
        "            ])\n",
        "\n",
        "            # Normalized feature vector\n",
        "            norm_combined = np.array([\n",
        "                (open_row[i] / close_row[idm],\n",
        "                low_row[i] / close_row[idm],\n",
        "                high_row[i] / close_row[idm],\n",
        "                close_row[i] / close_row[idm])\n",
        "                for i in range(window_size)\n",
        "            ]).flatten()\n",
        "\n",
        "            label = 1 if close_row[-1] > close_row[idm] else 0\n",
        "\n",
        "            data_splits[split]['features'].append(combined)\n",
        "            data_splits[split]['norm_features'].append(norm_combined)\n",
        "            data_splits[split]['labels'].append(label)\n",
        "\n",
        "# Process test data (using full set of indices)\n",
        "start_date, end_date = date_ranges['test']\n",
        "index_data = {}\n",
        "for name, ticker in test_tickers.items():\n",
        "    if ticker.endswith('.SZ') or ticker.endswith('.SS'):\n",
        "        index_data[name] = yf.Ticker(ticker).history(start=start_date, end=end_date, auto_adjust=True)\n",
        "    else:\n",
        "        index_data[name] = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True)\n",
        "\n",
        "# Create feature vectors and labels for test data\n",
        "for index_name, data in index_data.items():\n",
        "    if data.empty:\n",
        "        continue\n",
        "\n",
        "    if isinstance(data.columns, pd.MultiIndex):\n",
        "        data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "    open_values = data['Open'].dropna().values\n",
        "    close_values = data['Close'].dropna().values\n",
        "    low_values = data['Low'].dropna().values\n",
        "    high_values = data['High'].dropna().values\n",
        "\n",
        "    for start in range(len(data) - window_size + 1):\n",
        "        open_row = open_values[start:start + window_size]\n",
        "        low_row = low_values[start:start + window_size]\n",
        "        high_row = high_values[start:start + window_size]\n",
        "        close_row = close_values[start:start + window_size]\n",
        "\n",
        "        combined = np.array([\n",
        "            val for i in range(window_size)\n",
        "            for val in (open_row[i], low_row[i], high_row[i], close_row[i])\n",
        "        ])\n",
        "\n",
        "        norm_combined = np.array([\n",
        "            (open_row[i] / close_row[idm],\n",
        "            low_row[i] / close_row[idm],\n",
        "            high_row[i] / close_row[idm],\n",
        "            close_row[i] / close_row[idm])\n",
        "            for i in range(window_size)\n",
        "        ]).flatten()\n",
        "\n",
        "        label = 1 if close_row[-1] > close_row[idm] else 0\n",
        "\n",
        "        data_splits['test']['features'].append(combined)\n",
        "        data_splits['test']['norm_features'].append(norm_combined)\n",
        "        data_splits['test']['labels'].append(label)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "train_features = np.array(data_splits['train']['features'])\n",
        "train_norm_features = np.array(data_splits['train']['norm_features'])\n",
        "train_labels = np.array(data_splits['train']['labels'])\n",
        "\n",
        "val_features = np.array(data_splits['val']['features'])\n",
        "val_norm_features = np.array(data_splits['val']['norm_features'])\n",
        "val_labels = np.array(data_splits['val']['labels'])\n",
        "\n",
        "test_features = np.array(data_splits['test']['features'])\n",
        "test_norm_features = np.array(data_splits['test']['norm_features'])\n",
        "test_labels = np.array(data_splits['test']['labels'])\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Train norm_features shape: {train_norm_features.shape}\")\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "\n",
        "print(f\"Validation features shape: {val_features.shape}\")\n",
        "print(f\"Validation norm_features shape: {val_norm_features.shape}\")\n",
        "print(f\"Validation labels shape: {val_labels.shape}\")\n",
        "\n",
        "print(f\"Test features shape: {test_features.shape}\")\n",
        "print(f\"Test norm_features shape: {test_norm_features.shape}\")\n",
        "print(f\"Test labels shape: {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# ============================================\n",
        "\n",
        "# ============================================\n",
        "def extract_normalized_daily_ohlc(matrices, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    all_windows = []\n",
        "\n",
        "    for vec in matrices:\n",
        "        reshaped = vec.reshape(-1, 4)  # shape: (11, 4)\n",
        "        for i in range(1, reshaped.shape[0] - 10 + 1):\n",
        "            window = reshaped[i:i + 10]  # shape: (5, 4)\n",
        "            flat = window.flatten()   # shape: (20,)\n",
        "\n",
        "            denominator = flat[-5]  #\n",
        "            if abs(denominator) < epsilon:\n",
        "                denominator = 1.0  #\n",
        "\n",
        "            normalized = flat / denominator\n",
        "            all_windows.append(normalized)\n",
        "\n",
        "    return np.array(all_windows)  # shape: [N_samples × 7, 20]\n",
        "\n",
        "train_days = extract_normalized_daily_ohlc(train_features)\n",
        "val_days = extract_normalized_daily_ohlc(val_features)\n",
        "test_days = extract_normalized_daily_ohlc(test_features)"
      ],
      "metadata": {
        "id": "CyQYtJLwLMPa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Vector Quantizer\n",
        "# -------------------------\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings=256, embedding_dim=16):\n",
        "        super().__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Codebook K × D\n",
        "        self.embedding = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n",
        "\n",
        "    def forward(self, z_e):\n",
        "        # z_e: (B, 40, D)\n",
        "        B, T, D = z_e.shape\n",
        "        flat = z_e.reshape(-1, D)   # (B*40, D)\n",
        "\n",
        "        # Compute L2 distances\n",
        "        distances = (\n",
        "            flat.pow(2).sum(1, keepdim=True)\n",
        "            - 2 * flat @ self.embedding.t()\n",
        "            + self.embedding.pow(2).sum(1)\n",
        "        )\n",
        "\n",
        "        # nearest codebook entry\n",
        "        encoding_inds = distances.argmin(dim=1)   # (B*40,)\n",
        "\n",
        "        # quantized vectors\n",
        "        z_q = self.embedding[encoding_inds].view(B, T, D)\n",
        "\n",
        "        # straight-through estimator\n",
        "        z_q_st = z_e + (z_q - z_e).detach()\n",
        "\n",
        "        # commitment loss\n",
        "        commit_loss = F.mse_loss(z_e, z_q.detach())\n",
        "\n",
        "        return z_q_st, encoding_inds.view(B, T), commit_loss\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Encoder: 40 → 40 tokens\n",
        "# -------------------------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim=16):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 40)\n",
        "        x = x.unsqueeze(-1)    # (B, 40, 1)\n",
        "        return self.linear(x)  # (B, 40, D)\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# VQ Tokenizer (trained)\n",
        "# -------------------------\n",
        "class VQTokenizer(nn.Module):\n",
        "    def __init__(self, num_embeddings=256, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(embed_dim)\n",
        "        self.vq = VectorQuantizer(num_embeddings, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_e = self.encoder(x)                # (B, 40, D)\n",
        "        z_q, token_ids, loss = self.vq(z_e)  # (B, 40), scalar loss\n",
        "        return token_ids, loss"
      ],
      "metadata": {
        "id": "GPPPP1K6LXCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data: (23092, 40)\n",
        "data = torch.tensor(train_days, dtype=torch.float32)\n",
        "\n",
        "model = VQTokenizer(256, 64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "for step in range(2000):\n",
        "    idx = torch.randint(0, len(data), (batch_size,))\n",
        "    x = data[idx]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    tokens, commit_loss = model(x)\n",
        "\n",
        "    loss = commit_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"step {step}, commit_loss = {loss.item():.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g5yWTVVLzqI",
        "outputId": "704efc95-7e87-4145-f186-a36bfa948b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, commit_loss = 1.180495\n",
            "step 100, commit_loss = 0.868355\n",
            "step 200, commit_loss = 0.639742\n",
            "step 300, commit_loss = 0.471031\n",
            "step 400, commit_loss = 0.345981\n",
            "step 500, commit_loss = 0.253924\n",
            "step 600, commit_loss = 0.184158\n",
            "step 700, commit_loss = 0.133614\n",
            "step 800, commit_loss = 0.096130\n",
            "step 900, commit_loss = 0.068217\n",
            "step 1000, commit_loss = 0.048288\n",
            "step 1100, commit_loss = 0.033921\n",
            "step 1200, commit_loss = 0.023306\n",
            "step 1300, commit_loss = 0.016256\n",
            "step 1400, commit_loss = 0.010887\n",
            "step 1500, commit_loss = 0.007637\n",
            "step 1600, commit_loss = 0.005222\n",
            "step 1700, commit_loss = 0.003448\n",
            "step 1800, commit_loss = 0.002445\n",
            "step 1900, commit_loss = 0.001705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_tokens(model, x):\n",
        "    with torch.no_grad():\n",
        "        z_e = model.encoder(x)\n",
        "        _, token_ids, _ = model.vq(z_e)\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "tokens = to_tokens(model, data)\n",
        "print(tokens.shape)       # torch.Size([23092, 40])\n",
        "print(tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc563rkaMS2G",
        "outputId": "b75663c1-805d-4be2-a96d-3582da72d7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([23092, 40])\n",
            "tensor([183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
            "        183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
            "        183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens.max(),tokens.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FHnuTI-QDEH",
        "outputId": "f3337a10-aa3b-4891-ef37-94a248d5f6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(183), tensor(183))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.vq.embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZvFEAGLMs-A",
        "outputId": "a36723e7-6ee3-40cb-f27c-87e569d0ec02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple_discrete_autoencoder_colab.py\n",
        "# -------------------------------------------------------------\n",
        "# A minimal discrete autoencoder for tabular data (N, 40) -> tokens.\n",
        "# Turns continuous vectors of length 40 into discrete tokens via\n",
        "# Gumbel-Softmax straight-through estimator.\n",
        "#\n",
        "# This is intentionally simple and Colab-friendly. Just paste and run.\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --------------------------\n",
        "# Hyperparams\n",
        "# --------------------------\n",
        "input_dim = 40       # each row has 40 dims\n",
        "hidden_dim = 128\n",
        "z_dim = 64           # continuous embedding dim before quantization\n",
        "num_tokens = 256     # discrete vocabulary size\n",
        "batch_size = 64\n",
        "lr = 1e-3\n",
        "epochs = 10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --------------------------\n",
        "# 1. Simple MLP encoder\n",
        "# --------------------------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, z_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, z_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --------------------------\n",
        "# 2. Gumbel-Softmax quantizer\n",
        "# --------------------------\n",
        "class GumbelQuantizer(nn.Module):\n",
        "    def __init__(self, z_dim, num_tokens):\n",
        "        super().__init__()\n",
        "        self.logits = nn.Linear(z_dim, num_tokens)   # map latent -> token logits\n",
        "        self.embed = nn.Parameter(torch.randn(num_tokens, z_dim) * 0.1)\n",
        "\n",
        "    def forward(self, z, temp=1.0, hard=True):\n",
        "        # z: (B, z_dim)\n",
        "        logits = self.logits(z)            # (B, num_tokens)\n",
        "        g = F.gumbel_softmax(logits, tau=temp, hard=hard, dim=-1)\n",
        "        # convert one-hot to embedding\n",
        "        z_q = g @ self.embed               # (B, z_dim)\n",
        "        return z_q, logits                 # return continuous quantized and logits\n",
        "\n",
        "# --------------------------\n",
        "# 3. Decoder\n",
        "# --------------------------\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, out_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --------------------------\n",
        "# 4. Discrete Autoencoder wrapper\n",
        "# --------------------------\n",
        "class DiscreteAutoencoder(nn.Module):\n",
        "    def __init__(self, in_dim=40):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_dim, hidden_dim, z_dim)\n",
        "        self.quant = GumbelQuantizer(z_dim, num_tokens)\n",
        "        self.decoder = Decoder(z_dim, hidden_dim, in_dim)\n",
        "\n",
        "    def forward(self, x, temp=1.0):\n",
        "        z = self.encoder(x)\n",
        "        z_q, logits = self.quant(z, temp=temp, hard=True)\n",
        "        x_hat = self.decoder(z_q)\n",
        "        return x_hat, logits\n",
        "\n",
        "# --------------------------\n",
        "\n",
        "# --------------------------\n",
        "# Suppose your real data is a torch.tensor of shape (23092, 40)\n",
        "real_data = torch.tensor(train_days, dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(real_data)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# --------------------------\n",
        "# Training\n",
        "# --------------------------\n",
        "model = DiscreteAutoencoder(input_dim).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for (x,) in dataloader:\n",
        "        x = x.to(device)\n",
        "        opt.zero_grad()\n",
        "        x_hat, logits = model(x, temp=0.7)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss = {total_loss/len(dataset):.6f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# --------------------------\n",
        "# Convert the 40-d vectors into discrete tokens\n",
        "# --------------------------\n",
        "# After training, we can obtain discrete token ids by argmax over logits.\n",
        "\n",
        "def encode_to_tokens(model, data):\n",
        "    \"\"\"\n",
        "    data: (N, 40)\n",
        "    return: token_ids: (N,) integers in [0, num_tokens-1]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encoder(data.to(device))\n",
        "        logits = model.quant.logits(z)  # (N, num_tokens)\n",
        "        token_ids = torch.argmax(logits, dim=-1)\n",
        "        return token_ids.cpu()\n",
        "\n",
        "# Example: get token ids for the first 5 rows\n",
        "sample = real_data[:5]\n",
        "tokens = encode_to_tokens(model, sample)\n",
        "print(\"Discrete tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO6uXB27PkPN",
        "outputId": "17a06bd3-45cf-4d5a-f5bb-68141f501c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss = 0.069419\n",
            "Epoch 2, Loss = 0.001040\n",
            "Epoch 3, Loss = 0.001040\n",
            "Epoch 4, Loss = 0.001042\n",
            "Epoch 5, Loss = 0.001045\n",
            "Epoch 6, Loss = 0.001047\n",
            "Epoch 7, Loss = 0.001058\n",
            "Epoch 8, Loss = 0.001060\n",
            "Epoch 9, Loss = 0.001062\n",
            "Epoch 10, Loss = 0.001075\n",
            "Training finished.\n",
            "Discrete tokens: tensor([130, 130, 130, 130, 130])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = encode_to_tokens(model, real_data)"
      ],
      "metadata": {
        "id": "XkSvmN-gQhBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens.max(),tokens.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKt2343EQx3J",
        "outputId": "b14a804e-32d4-4e4e-a1c6-f8cfb2264184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(130), tensor(130))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}